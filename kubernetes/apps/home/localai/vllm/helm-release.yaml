# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &appname vllm
  namespace: &namespace home
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.1.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    defaultPodOptions:
      nodeSelector:
        node-role.kubernetes.io/worker: 'true'
    controllers:
      app:
        type: deployment
        annotations:
          reloader.stakater.com/auto: 'true'
        containers:
          app:
            image:
              repository: vllm/vllm-openai
              tag: v0.8.4@sha256:b168cbb0101f51b2491047345d0a83f5a8ecbe56a6604f2a2edb81eca55ebc9e
            command: ["/bin/sh", "-c"]
            args: [
              "vllm serve Qwen/Qwen2.5-VL-7B-Instruct-AWQ --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024"
            ]
            env:
              - name: TZ
                value: ${TIMEZONE}
            envFrom:
              - secretRef:
                  name: *appname 
            probes:
              liveness: &probe
                enabled: false
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8000
                  initialDelaySeconds: 120
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
              readiness: *probe
            resources:
              requests:
                cpu: 15m
                memory: 10G
                nvidia.com/gpu: "1"
              limits:
                nvidia.com/gpu: "1"

    persistence:
      data:
        enabled: true
        type: persistentVolumeClaim
        storageClass: openebs-hostpath
        accessMode: ReadWriteOnce
        size: 50Gi
        globalMounts:
          - path: /root/.cache/huggingface
      shm:
        type: emptyDir
        medium: Memory
        sizeLimit: 8Gi
        globalMounts:
          - path: /dev/shm
    service:
      app:
        ports:
          http:
            port: 8000
        primary: true
        controller: app
        type: LoadBalancer
    ingress:
      app:
        enabled: true
        className: traefik
        annotations:
          hajimari.io/enable: 'true'
          hajimari.io/icon: baby-bottle-outline
          hajimari.io/info: ChatGPT
          hajimari.io/group: &namespace home
          cert-manager.io/cluster-issuer: letsencrypt-production
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.middlewares: networking-chain-authelia@kubernetescrd
        hosts:
          - host: &uri ai.${SECRET_DEV_DOMAIN}
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app 
                  port: http
        tls:
          - hosts:
              - *uri
            secretName: *uri
    # podSecurityContext:
    #   runAsUser: 1000
    #   runAsGroup: 1000
    #   fsGroup: 1000
    #   fsGroupChangePolicy: "OnRootMismatch"
