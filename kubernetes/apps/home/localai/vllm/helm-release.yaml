# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &appname vllm
  namespace: &namespace home
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.1.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    defaultPodOptions:
      nodeSelector:
        node-role.kubernetes.io/worker: 'true'
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "special"
          effect: "NoSchedule"
    controllers:
      app:
        type: deployment
        annotations:
          reloader.stakater.com/auto: 'true'
        containers:
          app:
            image:
              repository: vllm/vllm-openai
              tag: v0.8.4@sha256:b168cbb0101f51b2491047345d0a83f5a8ecbe56a6604f2a2edb81eca55ebc9e
            command: ["/bin/sh", "-c"]
            args:
              - |
                vllm serve Qwen/Qwen2.5-VL-7B-Instruct-AWQ \
                  --trust-remote-code \
                  --enable-chunked-prefill \
                  --max_num_batched_tokens 1024 \
                  --served-model-name qwen2.5-vl \
                  --tensor-parallel-size 1 \
                  --tool-call-parser hermes \
                  --enable-auto-tool-choice \
                  --max-model-len 32768 \
                  --gpu_memory_utilization 0.90
            env:
              - name: TZ
                value: ${TIMEZONE}
              - name: CUDA_LAUNCH_BLOCKING
                value: 1
              - name: TORCH_USE_CUDA_DSA
                value: 1
            envFrom:
              - secretRef:
                  name: *appname 
            probes:
              liveness: &probe
                enabled: false
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8000
                  initialDelaySeconds: 120
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
              readiness: *probe
            resources:
              requests:
                cpu: 15m
                memory: 10G
                nvidia.com/gpu: "1"
              limits:
                nvidia.com/gpu: "1"

    persistence:
      data:
        enabled: true
        type: persistentVolumeClaim
        storageClass: openebs-hostpath
        accessMode: ReadWriteOnce
        size: 50Gi
        globalMounts:
          - path: /root/.cache/huggingface
      shm:
        type: emptyDir
        medium: Memory
        sizeLimit: 8Gi
        globalMounts:
          - path: /dev/shm
    service:
      app:
        ports:
          http:
            port: 8000
        primary: true
        controller: app
        type: LoadBalancer
    # podSecurityContext:
    #   runAsUser: 1000
    #   runAsGroup: 1000
    #   fsGroup: 1000
    #   fsGroupChangePolicy: "OnRootMismatch"
