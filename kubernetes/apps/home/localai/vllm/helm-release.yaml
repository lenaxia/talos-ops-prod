# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &appname vllm
  namespace: &namespace home
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.1.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    defaultPodOptions:
      nodeSelector:
        node-role.kubernetes.io/worker: 'true'
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "special"
          effect: "NoSchedule"
    controllers:
      app:
        type: deployment
        annotations:
          reloader.stakater.com/auto: 'true'
        containers:
          app:
            image:
              repository: vllm/vllm-openai
              tag: v0.11.0@sha256:014a95f21c9edf6abe0aea6b07353f96baa4ec291c427bb1176dc7c93a85845c
            command: ["/bin/sh", "-c"]
            args:
              - |
                vllm serve \
                    QuantTrio/Qwen3-VL-32B-Instruct-AWQ \
                    --served-model-name qwen3-vl \
                    --swap-space 4 \
                    --max-num-seqs 8 \
                    --max-model-len 40960 \
                    --gpu-memory-utilization 0.9 \
                    --tensor-parallel-size 1 \
                    --trust-remote-code \
                    --disable-log-requests \
                    --host 0.0.0.0 \
                    --enable-chunked-prefill \
                    --enforce-eager 
                    --port 8000
            env:
              - name: TZ
                value: ${TIMEZONE}
              - name: CUDA_LAUNCH_BLOCKING
                value: 1
              - name: TORCH_USE_CUDA_DSA
                value: 1
              - name: NVIDIA_VISIBLE_DEVICES
                value: "GPU-ec4bf65c-4e9e-b857-11e7-bcc716f11d93" # 4090
              #  value: "GPU-755d8528-c8c7-5fd4-f441-f061368f4547"
            envFrom:
              - secretRef:
                  name: *appname 
            probes:
              liveness: &probe
                enabled: false
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8000
                  initialDelaySeconds: 120
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
              readiness: *probe
            resources:
              requests:
                nvidia.com/gpu: "3"
              limits:
                nvidia.com/gpu: "3"

    persistence:
      data:
        enabled: true
        type: persistentVolumeClaim
        storageClass: openebs-hostpath
        accessMode: ReadWriteOnce
        size: 50Gi
        globalMounts:
          - path: /root/.cache/huggingface
      shm:
        type: emptyDir
        medium: Memory
        sizeLimit: 8Gi
        globalMounts:
          - path: /dev/shm
    service:
      app:
        ports:
          http:
            port: 8000
        primary: true
        controller: app
        type: LoadBalancer
    # podSecurityContext:
    #   runAsUser: 1000
    #   runAsGroup: 1000
    #   fsGroup: 1000
    #   fsGroupChangePolicy: "OnRootMismatch"
