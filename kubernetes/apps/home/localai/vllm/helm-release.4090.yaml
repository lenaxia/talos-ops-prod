# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &appname vllm-4090
  namespace: &namespace home
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.7.3
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    defaultPodOptions:
      nodeSelector:
        node-role.kubernetes.io/worker: 'true'
    controllers:
      app:
        type: deployment
        annotations:
          reloader.stakater.com/auto: 'true'
        containers:
          app:
            image:
              repository: vllm/vllm-openai
              tag: v0.11.0@sha256:014a95f21c9edf6abe0aea6b07353f96baa4ec291c427bb1176dc7c93a85845c
            command: ["/bin/sh", "-c"]
            args: [
              "vllm serve Qwen/Qwen2.5-72B-Instruct-AWQ --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024 --served-model-name default Qwen/Qwen2.5-72B-Instruct-AWQ"
            ]
            env:
              - name: TZ
                value: ${TIMEZONE}
              - name: NVIDIA_VISIBLE_DEVICES
                value: 2
            envFrom:
              - secretRef:
                  name: vllm
            probes:
              liveness: &probe
                enabled: false
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8000
                  initialDelaySeconds: 120
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
              readiness: *probe
            resources:
              requests:
                cpu: 15m
                memory: 10G
                nvidia.com/gpu: "1"
              limits:
                nvidia.com/gpu: "1"

    persistence:
      data:
        enabled: true
        type: persistentVolumeClaim
        storageClass: openebs-hostpath
        accessMode: ReadWriteOnce
        size: 50Gi
        globalMounts:
          - path: /root/.cache/huggingface
      shm:
        type: emptyDir
        medium: Memory
        sizeLimit: 8Gi
        globalMounts:
          - path: /dev/shm
    service:
      app:
        ports:
          http:
            port: 8000
        primary: true
        controller: app
        type: LoadBalancer
    # podSecurityContext:
    #   runAsUser: 1000
    #   runAsGroup: 1000
    #   fsGroup: 1000
    #   fsGroupChangePolicy: "OnRootMismatch"
