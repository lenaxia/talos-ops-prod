# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &appname litellm
  namespace: &namespace home
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 4.3.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    defaultPodOptions:
      nodeSelector:
        node-role.kubernetes.io/worker: 'true'
    controllers:
      app:
        type: statefulset
        annotations:
          reloader.stakater.com/auto: 'true'
        initContainers:
          init-db:
            env:
              - name: INIT_POSTGRES_SUPER_PASS
                valueFrom:
                  secretKeyRef:
                    name: postgres-superuser
                    key: password
              - name: INIT_POSTGRES_HOST
                valueFrom:
                  secretKeyRef:
                    name: postgres-superuser
                    key: host
            envFrom:
              - secretRef:
                  name: *appname
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: '17.4'
        containers:
          app:
            image:
              repository: ghcr.io/berriai/litellm
              tag: v1.80.8-stable
            args:
              - "--config"
              - "/app/proxy_server_config.yaml"
            env:
              - name: LITELLM_LICENSE
                value: 
                  MmNoSEtTRXBEYmVEZXNLYjdWcGVKaHVrQUxwang4SG16TU0yMnJYeXlZWGswMk9HSVNLS0N4WDUzOVBWM3FZUwo=
              - name: TZ
                #value: ${TIMEZONE}
                value: 'America/Los_Angeles'
              - name: DISABLE_SCHEMA_UPDATE
                value: "true"
              - name: INIT_POSTGRES_HOST
                valueFrom:
                  secretKeyRef:
                    name: postgres-superuser
                    key: host
              - name: DATABASE_URL
                value: "postgresql://$(INIT_POSTGRES_USER):$(INIT_POSTGRES_PASS)@$(INIT_POSTGRES_HOST):5432/$(INIT_POSTGRES_DBNAME)"
              - name: UI_LOGO_PATH
                value: "https://s3.thekao.cloud/public/customlogo.png"
              - name: STORE_MODEL_IN_DB
                value: true
            envFrom:
              - secretRef:
                  name: *appname
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/liveliness
                    port: 4000
                  initialDelaySeconds: 120
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/readiness
                    port: 4000
                  initialDelaySeconds: 20
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
            resources:
              requests:
                cpu: 15m
                memory: 94M

    configMaps:
      config-yaml:
        enabled: true
        data:
          config.yaml: |
            litellm_settings:
              set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
              drop_params: True
              request_timeout: 600    # raise Timeout error if call takes longer than 600 seconds. Default value is 6000seconds if not set
              set_verbose: False      # Switch off Debug Logging, ensure your logs do not have any debugging on
              json_logs: true         # Get debug logs in json format

            general_settings:
              master_key: sk-1234      # enter your own master key, ensure it starts with 'sk-'
              #alerting: ["slack"]      # Setup slack alerting - get alerts on LLM exceptions, Budget Alerts, Slow LLM Responses
              proxy_batch_write_at: 60 # Batch write spend updates every 60s
              database_connection_pool_limit: 10 # limit the number of database connections to = MAX Number of DB Connections/Number of instances of litellm proxy (Around 10-20 is good number)
              allow_requests_on_db_unavailable: True

              # OPTIONAL Best Practices
              disable_spend_logs: True # turn off writing each transaction to the db. We recommend doing this is you don't need to see Usage on the LiteLLM UI and are tracking metrics via Prometheus
              disable_error_logs: True # turn off writing LLM Exceptions to DB


            model_list: 
              ## LocalAI
              #- model_name: localai-llama3.1-q8-functioncall
              #  litellm_params:
              #    model: openai/meta-llama-3.1-8b-instruct:Q8_grammar-functioncall
              #    api_base: http://localai.home.svc.cluster.local:8080
              #    api_key: doesnotmatter

              # Tabby API 
              #- model_name: default
              #  litellm_params:
              #    model: openai/Qwen2.5-72B-Instruct-exl2-4_25
              #    api_base: http://tabbyapi.home.svc.cluster.local:5000/v1
              #    api_key: doesnotmatter
              #  model_info:
              #    input_cost_per_token: 0.00001
              #    output_cost_per_token: 0.00001
              #    max_tokens: 40960
              #- model_name: exl2-qwen2.5-72b-instruct_4_25
              #  litellm_params:
              #    model: openai/Qwen2.5-72B-Instruct-exl2-4_25
              #    api_base: http://tabbyapi.default.svc.cluster.local:5000/v1
              #    api_key: doesnotmatter
              #  model_info:
              #    input_cost_per_token: 0.00001
              #    output_cost_per_token: 0.00001
              #    max_tokens: 40960

              # vLLM
              #- model_name: default
              #  litellm_params:
              #    model: hosted_vllm/default
              #    api_base: http://vllm.home.svc.cluster.local:8000/v1
              #    api_key: doesnotmatter
              #  model_info:
              #    input_cost_per_token: 0.00001
              #    output_cost_per_token: 0.00001
              #    max_tokens: 40960
              #- model_name: qwen3-30a3b-test
              #  litellm_params:
              #    model: openai/cognitivecomputations/Qwen3-30B-A3B-AWQ
              #    api_base: http://vllm.home.svc.cluster.local:8000/v1
              #    api_key: doesnotmatter
              #  model_info:
              #    input_cost_per_token: 0.00001
              #    output_cost_per_token: 0.00001
              #    max_tokens: 40960
              - model_name: default
                litellm_params:
                  model: openai/qwen3-vl
                  api_base: http://vllm.home.svc.cluster.local:8000/v1
                  api_key: doesnotmatter
                model_info:
                  input_cost_per_token: 0.00001
                  output_cost_per_token: 0.00001
                  max_tokens: 100000
                  supports_tool_choice: true

              # Deepseek
              - model_name: deepseek-v3-chat
                litellm_params:
                  model: deepseek/deepseek-chat
                model_info:
                  supports_tool_choice: true
              - model_name: deepseek-r1-reasoning
                litellm_params:
                  model: deepseek/deepseek-reasoner

              - model_name: glm-4.7
                litellm_params:
                  model: zai/glm-4.7
              - model_name: glm-4.6
                litellm_params:
                  model: zai/glm-4.6
            

              # OpenAI
              - model_name: o3-mini-reasoning
                litellm_params:
                  model: openai/o3-mini
              - model_name: gpt-4o
                litellm_params:
                  model: openai/gpt-4o
              - model_name: gpt-4o-mini
                litellm_params:
                  model: openai/gpt-4o-mini
              - model_name: dalle3
                litellm_params:
                  model: openai/dall-e-3

              # Bedrock
              - model_name: bedrock-claude-sonnet-4.5
                litellm_params:
                  model: bedrock/global.anthropic.claude-sonnet-4-5-20250929-v1:0
                  aws_region_name: us-west-2
                  aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
                  aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
                  aws_region_name: os.environ/AWS_REGION_NAME
              #- model_name: bedrock-claude-v2-haiku
              #  litellm_params:
              #    model: bedrock/anthropic.claude-3-haiku-20240307-v1:0
              #    aws_region_name: us-west-2
              #- model_name: bedrock-claude-v2-opus
              #  litellm_params:
              #    model: bedrock/anthropic.claude-3-opus-20240229-v1:0
              #    aws_region_name: us-west-2
              #- model_name: bedrock-mixtral-8x7b
              #  litellm_params:
              #    model: bedrock/mistral.mixtral-8x7b-instruct-v0:1
              #    aws_region_name: us-west-2
              #- model_name: bedrock-llama3-70b
              #  litellm_params:
              #    model: bedrock/meta.llama3-70b-instruct-v1:0
              #    aws_region_name: us-west-2
              #- model_name: bedrock-command-r-plus
              #  litellm_params:
              #    model: bedrock/cohere.command-r-plus-v1:0
              #    aws_region_name: us-west-2
    persistence:
      config-yaml:
        enabled: true
        type: configMap
        name: litellm
        globalMounts:
          - path: /app/proxy_server_config.yaml
            subPath: config.yaml
    service:
      app:
        ports:
          http:
            port: 4000
        primary: true
        type: LoadBalancer
    ingress:
      app:
        enabled: true
        className: traefik
        annotations:
          hajimari.io/enable: 'true'
          hajimari.io/icon: baby-bottle-outline
          hajimari.io/info: ChatGPT
          hajimari.io/group: &namespace home
          cert-manager.io/cluster-issuer: letsencrypt-production
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.middlewares: 
            networking-chain-authelia@kubernetescrd
        hosts:
          #- host: &uri ai.${SECRET_DEV_DOMAIN}
          - host: &uri ai.thekao.cloud
            paths:
              - path: /
                pathType: Prefix
                service:
                  port: http
        tls:
          - hosts:
              - *uri
            secretName: *uri
    # podSecurityContext:
    #   runAsUser: 1000
    #   runAsGroup: 1000
    #   fsGroup: 1000
    #   fsGroupChangePolicy: "OnRootMismatch"
