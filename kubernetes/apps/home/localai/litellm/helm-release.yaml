# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &appname litellm
  namespace: &namespace home
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.1.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    defaultPodOptions:
      nodeSelector:
        node-role.kubernetes.io/worker: 'true'
    controllers:
      app:
        type: statefulset
        annotations:
          reloader.stakater.com/auto: 'true'
        initContainers:
          init-db:
            env:
              - name: INIT_POSTGRES_SUPER_PASS
                valueFrom:
                  secretKeyRef:
                    name: postgres-superuser
                    key: password
              - name: INIT_POSTGRES_HOST
                valueFrom:
                  secretKeyRef:
                    name: postgres-superuser
                    key: host
            envFrom:
              - secretRef:
                  name: *appname
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: '16.8'
        containers:
          app:
            image:
              repository: ghcr.io/berriai/litellm
              tag: v1.72.6-stable
            args:
              - "--config"
              - "/app/proxy_server_config.yaml"
            env:
              - name: LITELLM_LICENSE
                value: MmNoSEtTRXBEYmVEZXNLYjdWcGVKaHVrQUxwang4SG16TU0yMnJYeXlZWGswMk9HSVNLS0N4WDUzOVBWM3FZUwo=
              - name: TZ
                #value: ${TIMEZONE}
                value: 'America/Los_Angeles'
              - name: DISABLE_SCHEMA_UPDATE
                value: "true"
              - name: INIT_POSTGRES_HOST
                valueFrom:
                  secretKeyRef:
                    name: postgres-superuser
                    key: host
              - name: DATABASE_URL
                value: "postgresql://$(INIT_POSTGRES_USER):$(INIT_POSTGRES_PASS)@$(INIT_POSTGRES_HOST):5432/$(INIT_POSTGRES_DBNAME)"
              - name: UI_LOGO_PATH
                value: "https://s3.thekao.cloud/public/customlogo.png"
              - name: STORE_MODEL_IN_DB
                value: true
            envFrom:
              - secretRef:
                  name: *appname 
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/liveliness
                    port: 4000
                  initialDelaySeconds: 120
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
              readiness:
                enabled: true
                custom: true
                spec: 
                  httpGet:
                    path: /health/readiness
                    port: 4000
                  initialDelaySeconds: 20
                  periodSeconds: 15
                  successThreshold: 1
                  failureThreshold: 3
                  timeoutSeconds: 10
            resources:
              requests:
                cpu: 15m
                memory: 94M

    configMaps:
      config-yaml:
        enabled: true
        data:
          config.yaml: |
              litellm_settings:
                set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
                #drop_params: True
                request_timeout: 600    # raise Timeout error if call takes longer than 600 seconds. Default value is 6000seconds if not set
                set_verbose: False      # Switch off Debug Logging, ensure your logs do not have any debugging on
                json_logs: true         # Get debug logs in json format
              
              general_settings:
                master_key: sk-1234      # enter your own master key, ensure it starts with 'sk-'
                #alerting: ["slack"]      # Setup slack alerting - get alerts on LLM exceptions, Budget Alerts, Slow LLM Responses
                proxy_batch_write_at: 60 # Batch write spend updates every 60s
                database_connection_pool_limit: 10 # limit the number of database connections to = MAX Number of DB Connections/Number of instances of litellm proxy (Around 10-20 is good number)
                allow_requests_on_db_unavailable: True
              
                # OPTIONAL Best Practices
                disable_spend_logs: True # turn off writing each transaction to the db. We recommend doing this is you don't need to see Usage on the LiteLLM UI and are tracking metrics via Prometheus
                disable_error_logs: True # turn off writing LLM Exceptions to DB


              model_list: 
                ## LocalAI
                #- model_name: localai-llama3.1-q8-functioncall
                #  litellm_params:
                #    model: openai/meta-llama-3.1-8b-instruct:Q8_grammar-functioncall
                #    api_base: http://localai.home.svc.cluster.local:8080
                #    api_key: doesnotmatter

                # Tabby API 
                - model_name: default
                  litellm_params:
                    model: openai/Qwen2.5-72B-Instruct-exl2-4_25
                    api_base: http://tabbyapi.default.svc.cluster.local:5000/v1
                    api_key: doesnotmatter
                  model_info:
                    input_cost_per_token: 0.00001
                    output_cost_per_token: 0.00001
                    max_tokens: 40960
                #- model_name: exl2-qwen2.5-72b-instruct_4_25
                #  litellm_params:
                #    model: openai/Qwen2.5-72B-Instruct-exl2-4_25
                #    api_base: http://tabbyapi.default.svc.cluster.local:5000/v1
                #    api_key: doesnotmatter
                #  model_info:
                #    input_cost_per_token: 0.00001
                #    output_cost_per_token: 0.00001
                #    max_tokens: 40960

                # vLLM
                #- model_name: default
                #  litellm_params:
                #    model: hosted_vllm/default
                #    api_base: http://vllm.home.svc.cluster.local:8000/v1
                #    api_key: doesnotmatter
                #  model_info:
                #    input_cost_per_token: 0.00001
                #    output_cost_per_token: 0.00001
                #    max_tokens: 40960
                #- model_name: qwen3-30a3b-test
                #  litellm_params:
                #    model: openai/cognitivecomputations/Qwen3-30B-A3B-AWQ
                #    api_base: http://vllm.home.svc.cluster.local:8000/v1
                #    api_key: doesnotmatter
                #  model_info:
                #    input_cost_per_token: 0.00001
                #    output_cost_per_token: 0.00001
                #    max_tokens: 40960
                - model_name: qwen2.5-vl
                  litellm_params:
                    model: openai/Qwen/Qwen2.5-VL-7B-Instruct-AWQ
                    api_base: http://vllm.home.svc.cluster.local:8000/v1
                    api_key: doesnotmatter
                  model_info:
                    input_cost_per_token: 0.00001
                    output_cost_per_token: 0.00001
                    max_tokens: 40960

                # Deepseek
                - model_name: deepseek-v3-chat
                  litellm_params:
                    model: deepseek/deepseek-chat
                - model_name: deepseek-r1-reasoning
                  litellm_params:
                    model: deepseek/deepseek-reasoner

                # OpenAI
                - model_name: o3-mini-reasoning
                  litellm_params:
                    model: openai/o3-mini
                - model_name: gpt-4o
                  litellm_params:
                    model: openai/gpt-4o
                - model_name: gpt-4o-mini
                  litellm_params:
                    model: openai/gpt-4o-mini
                - model_name: dalle3
                  litellm_params:
                    model: openai/dall-e-3

                ## Bedrock
                #- model_name: bedrock-claude-sonnet-3-5
                #  litellm_params:
                #    model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0
                #    aws_region_name: us-east-1
                #- model_name: bedrock-claude-v2-sonnet
                #  litellm_params:
                #    model: bedrock/anthropic.claude-3-sonnet-20240229-v1:0
                #    aws_region_name: us-west-2
                #- model_name: bedrock-claude-v2-haiku
                #  litellm_params:
                #    model: bedrock/anthropic.claude-3-haiku-20240307-v1:0
                #    aws_region_name: us-west-2
                #- model_name: bedrock-claude-v2-opus
                #  litellm_params:
                #    model: bedrock/anthropic.claude-3-opus-20240229-v1:0
                #    aws_region_name: us-west-2
                #- model_name: bedrock-mixtral-8x7b
                #  litellm_params:
                #    model: bedrock/mistral.mixtral-8x7b-instruct-v0:1
                #    aws_region_name: us-west-2
                #- model_name: bedrock-llama3-70b
                #  litellm_params:
                #    model: bedrock/meta.llama3-70b-instruct-v1:0
                #    aws_region_name: us-west-2
                #- model_name: bedrock-command-r-plus
                #  litellm_params:
                #    model: bedrock/cohere.command-r-plus-v1:0
                #    aws_region_name: us-west-2
    persistence:
      config-yaml:
        enabled: true
        type: configMap
        name: litellm-config-yaml
        globalMounts:
          - path: /app/proxy_server_config.yaml
            subPath: config.yaml
    service:
      app:
        ports:
          http:
            port: 4000
        primary: true
        controller: app
        type: LoadBalancer
    ingress:
      app:
        enabled: true
        className: traefik
        annotations:
          hajimari.io/enable: 'true'
          hajimari.io/icon: baby-bottle-outline
          hajimari.io/info: ChatGPT
          hajimari.io/group: &namespace home
          cert-manager.io/cluster-issuer: letsencrypt-production
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.middlewares: networking-chain-authelia@kubernetescrd
        hosts:
          #- host: &uri ai.${SECRET_DEV_DOMAIN}
          - host: &uri ai.thekao.cloud
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app 
                  port: http
        tls:
          - hosts:
              - *uri
            secretName: *uri
    # podSecurityContext:
    #   runAsUser: 1000
    #   runAsGroup: 1000
    #   fsGroup: 1000
    #   fsGroupChangePolicy: "OnRootMismatch"
