# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v3beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: tabbyapi
  namespace: home
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 2.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  values:
    defaultPodOptions:
      nodeSelector:
        node-role.kubernetes.io/worker: 'true'

    controllers:
      main:
        initContainers:
          - name: download-model
            image: 
              repository: "python"
              tag: "3.13-slim"
            command: ["/bin/bash", "-c"]
            args:
              - |
                pip install --no-cache-dir huggingface_hub
                python - <<'EOF'
                import os
                from huggingface_hub import hf_hub_download
        
                model_dir = "/app/models/Qwen2.5-72B-Instruct-exl2-4_25"
                os.makedirs(model_dir, exist_ok=True)
        
                files = ["config.json", "tokenizer.json", "output-00001-of-00005.safetensors"]
                for f in files:
                    hf_hub_download(
                        repo_id="bartowski/Qwen2.5-72B-Instruct-exl2",
                        filename=f,
                        revision="4_25",
                        local_dir=model_dir
                    )
                EOF
            volumeMounts:
              - name: models
                mountPath: /app/models
        containers:
          main:
            image:
              repository: ghcr.io/lenaxia/tabbyapi
              tag: 2025.10.25
            env:
              - name: TZ
                value: America/Los_Angeles
              - name: NVIDIA_VISIBLE_DEVICES
                value: "GPU-ec4bf65c-4e9e-b857-11e7-bcc716f11d93"
            resources:
              requests:
                nvidia.com/gpu: 1
                cpu: 200m
                memory: 2000Mi
              limits:
                nvidia.com/gpu: 1
                memory: 40000Mi
            volumeMounts:
              - name: models
                mountPath: /app/models
              - name: config
                mountPath: /app/config.yml
                subPath: config.yml

    service:
      main:
        type: LoadBalancer
        ports:
          http:
            port: 5000

    ingress:
      main:
        enabled: true
        annotations:
          hajimari.io/enable: 'true'
          cert-manager.io/cluster-issuer: letsencrypt-production
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
        hosts:
          - host: tabby.thekao.cloud
            paths:
              - path: /
                pathType: Prefix
                service:
                  name: main
                  port: http
        tls:
          - hosts:
              - tabby.thekao.cloud
            secretName: tabby.thekao.cloud

    persistence:
      configyml:
        enabled: true
        type: configMap
        name: tabbyapi
        defaultMode: 0644
      models:
        enabled: true
        existingClaim: tabbyapi-models

